+++ 
draft = false
date = 2026-02-02T21:41:58+08:00
title = "[毕业设计]Day2"
description = ""
slug = "dissertation-day2"
authors = []
tags = []
categories = []
externalLink = ""
series = []
+++

##  验证采样偏差($\text{sampling bias}$)

### 术语规定

有 $N$ 个结点：$n_1,n_2,...n_N$ ，每个结点 $n_i$ 对应一个特征向量 $X_i$ 表示该结点自身特征，一个邻接向量 $A_i$ 表示该结点与其他结点的联系。所有的特征向量 $X_{1-N}$ 组成特征矩阵 $X$ ，所有的邻接向量 $A_{1-N}$ 组成邻接矩阵 $A$ ，图 $G = (X,A)$ 即
$$
X = \begin{pmatrix}
X_1 \\
X_2 \\
... \\
X_N
\end{pmatrix}
\qquad
A = \begin{pmatrix}
A_1 \\
A_2 \\
... \\
A_N
\end{pmatrix}
\qquad
G = (X,A)
$$
图对比学习($\text{graph contrastive learning}$)的目的是训练得到一个编码器 $f_\theta$ ，它能将原图 $G$ 映射成一个 $N \times d$ 的矩阵 $H$ 
$$
H = \begin{pmatrix}
H_1 \\
H_2 \\
... \\
H_N
\end{pmatrix}
$$
原始结点 $n_i$ 对应编码向量 $H_i$ ，$H_i$ 与 $H_j$ 的距离反映了结点 $n_i$ 与 $n_j$ 的语义相似度，距离越小，两结点的语义越相似

图对比学习就是要得到能够准确区分各结点语义，把语义相近的结点映射成距离接近的编码向量的编码器 $f_\theta$ 

特征掩蔽($\text{feature masking}$)的作用是改变结点 $n_i$ 的特征向量 $X_i$ 得到原图 $G$ 的增强视图 $G^{aug1} = (X^{aug1},A)$ ，可以将 $G^{aug1}$ 中的结点编号 $u_1,u_2,...u_N$ 与 $n_1,n_2,...n_N$ 一一对应，$G^{aug1}$ 经过编码器 $f_\theta$ 编码得到编码矩阵 $U$ ，结点 $u_i$ 的编码向量为 $U_i$  
$$
U = f_\theta(G^{aug1}) = \begin{pmatrix}
U_1 \\
U_2 \\
... \\
U_N
\end{pmatrix}
$$
边丢弃($\text{edge dropping}$)的作用是改变结点 $n_i$ 的邻接向量 $A_i$ 得到原图 $G$ 的增强视图 $G^{aug2} = (X,A^{aug2})$ ，可以将 $G^{aug2}$ 中的结点编号 $v_1,v_2,...v_N$ 与 $n_1,n_2,...n_N$ 一一对应，$G^{aug2}$ 经过编码器 $f_\theta$ 编码得到编码矩阵 $V$ ，结点 $v_i$ 的编码向量为 $V_i$  
$$
V = f_\theta(G^{aug2}) = \begin{pmatrix}
V_1 \\
V_2 \\
... \\
V_N
\end{pmatrix}
$$

### 采样偏差出现的原因

训练过程中损失函数 $L$
$$
\begin{aligned}
L &= \frac{1}{2N} \sum_{i=1}^N (l_{u_i, v_i} + l_{v_i, u_i}) \\
\text{其中}，l_{u_i, v_i} &= -\log \frac{s_{\theta}(u_i, v_i)}{\sum_{j \neq i, j=1}^N s_{\theta}(u_i, u_j) + \sum_{j=1}^N s_{\theta}(u_i, v_j)} \\ \\
s_{\theta}(u_i, v_j) &= \exp(\text{cos}(\mathbf{U}_i, \mathbf{V}_j) / \tau) 
\end{aligned}
$$
$s_{\theta}(u_i, v_j)$ 表示 $G^{aug1}$ 中结点 $u_i$ 与 $G^{aug2}$ 中结点 $v_j$ 的余弦相似度($\text{cosine similarity}$)，为了使损失 $L$ 最小， $f_\theta$ 要使自己编码得到的编码向量中 $U_i$ 与 $V_i$ 最接近，而 $U_i$ 与 $U_j$ , $V_j$ 都要尽可能远。换句话说，$f_\theta$ 需要实现让结点 $u_i$ 只与 $v_i$ 接近，远离其他一切结点，忽略了在原始结点 $n_1,n_2,...n_N$ 中可能本身就有语义极为接近甚至相同的结点，它们在经过特则掩蔽和边丢弃之后产生的结点也可能有相似的语义，即与结点 $u_i$ 语义接近的不只有 $v_i$ ，很可能还有其他的 $u_j,v_j$ 这一事实

### 实验设计

使用有标签数据进行监督学习，得到一个最能准确表述语义的编码器 $f_{\theta^*}$ ，计算结点 $u_{1-N}$ 与 $v_{1-N}$ 的相似度

**预期结果**：除 $u_i$ 与 $v_i$ 相似度高外，还有其他的 $v_j$ 与 $u_i$ 相似度高

### 下载数据集

下载数据集 $Cora$ 并查看它的具体信息：

```
==============================
Cora 数据集概览
==============================
图的数量: 1
类别数量 (Classes): 7
节点总数 (N): 2708
边总数 (E): 10556
特征维度 (d): 1433

==============================
矩阵结构详解
==============================
【特征矩阵 X】 data.x
  - 形状: torch.Size([2708, 1433])
  - 含义: [2708 个节点, 1433 维特征]
  - 示例 (第1个节点的特征前10位): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

【邻接矩阵 A (稀疏形式)】 data.edge_index
  - 形状: torch.Size([2, 10556])
  - 含义: [2 行, 10556 条边]
  - 解释: 第一行是起点索引，第二行是终点索引
  - 示例 (前5条边):
tensor([[ 633, 1862, 2582,    2,  652],
        [   0,    0,    0,    1,    1]])

【标签向量 Y】 data.y
  - 形状: torch.Size([2708])
  - 含义: 每个节点对应的类别索引 (0-6)
  - 示例 (前10个节点的标签): [3, 4, 4, 0, 3, 2, 0, 3, 3, 2]

【数据集划分】
  - 训练节点数: 140
  - 验证节点数: 500
  - 测试节点数: 1000
```

### 训练 $f_{\theta^*}$ 

```
--- 开始训练 ---
Epoch 010: 发现新高分 0.3780，模型已保存！
Epoch 020: 发现新高分 0.4560，模型已保存！
Epoch 030: 发现新高分 0.5540，模型已保存！
Epoch 050: 发现新高分 0.6140，模型已保存！
Epoch 060: 发现新高分 0.6560，模型已保存！
Epoch 080: 发现新高分 0.6680，模型已保存！
Epoch 180: 发现新高分 0.6760，模型已保存！

==============================
训练结束！
历史最高验证集准确率: 0.6760
```

训练好的编码器保存在 $model/$ 文件夹下

### 绘制热力图